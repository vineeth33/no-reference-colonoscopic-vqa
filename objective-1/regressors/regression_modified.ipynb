{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9321d6d-1556-4786-a451-48e5fa55f5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vineethu/Documents/GitHub/no-reference-colonoscopic-vqa/jupyter_env/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vineethu/Documents/GitHub/no-reference-colonoscopic-vqa/jupyter_env/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp313-cp313-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp313-cp313-macosx_12_0_arm64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.1-cp313-cp313-macosx_14_0_arm64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, numpy, joblib, scipy, pandas, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.1 numpy-2.3.2 pandas-2.3.1 pytz-2025.2 scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea2b15-4ab6-4153-8dd1-1f6ae6096957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "class RegressionModelTrainer:\n",
    "    \"\"\"\n",
    "    A class for training SVR and MLP regression models on the SVD feature dataset\n",
    "    with hyperparameter tuning and performance evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features_path, labels_path, models_save_path):\n",
    "        \"\"\"\n",
    "        Initialize the trainer with paths and configuration.\n",
    "\n",
    "        Args:\n",
    "            features_path: Path to the modified_svd_features.csv file\n",
    "            labels_path: Path to the labels CSV file\n",
    "            models_save_path: Base path to save trained models and results\n",
    "        \"\"\"\n",
    "        self.features_path = features_path\n",
    "        self.labels_path = labels_path\n",
    "        self.models_save_path = models_save_path\n",
    "\n",
    "        self.labels = ['TSV', 'B', 'SR', 'S', 'U', 'O']\n",
    "\n",
    "        self.models = {\n",
    "            'svr': {\n",
    "                'model': SVR,\n",
    "                'params': {\n",
    "                    'C': [0.1, 1, 10, 100],\n",
    "                    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "                    'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "                    'epsilon': [0.01, 0.1, 0.2]\n",
    "                }\n",
    "            },\n",
    "            'mlp': {\n",
    "                'model': MLPRegressor,\n",
    "                'params': {\n",
    "                    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
    "                    'activation': ['relu', 'tanh', 'logistic'],\n",
    "                    'solver': ['adam', 'sgd'],\n",
    "                    'alpha': [0.0001, 0.001, 0.01],\n",
    "                    'learning_rate': ['constant', 'adaptive'],\n",
    "                    'max_iter': [500, 1000]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.model_save_dirs = {\n",
    "            'svr': 'svr-2',\n",
    "            'mlp': 'mlp-2'\n",
    "        }\n",
    "\n",
    "        self._create_model_directories()\n",
    "        self.labels_df = self._load_labels()\n",
    "        self.results = []\n",
    "\n",
    "    def _create_model_directories(self):\n",
    "        \"\"\"Create specified directories for saving trained models.\"\"\"\n",
    "        print(\"Creating model directories...\")\n",
    "        for dir_name in self.model_save_dirs.values():\n",
    "            model_dir = os.path.join(self.models_save_path, dir_name)\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "        print(f\"Directories will be saved in: {os.path.abspath(self.models_save_path)}\\n\")\n",
    "\n",
    "    def _load_labels(self):\n",
    "        \"\"\"Load the labels CSV file.\"\"\"\n",
    "        try:\n",
    "            labels_df = pd.read_csv(self.labels_path)\n",
    "            print(f\"Labels loaded successfully from {self.labels_path}. Shape: {labels_df.shape}\\n\")\n",
    "            return labels_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading labels: {e}\\n\")\n",
    "            return None\n",
    "\n",
    "    def _load_features(self):\n",
    "        \"\"\"Load features from the specified SVD features CSV file.\"\"\"\n",
    "        try:\n",
    "            features_df = pd.read_csv(self.features_path)\n",
    "            if 'videoname' in features_df.columns:\n",
    "                features_df = features_df.drop('videoname', axis=1)\n",
    "            print(f\"Features loaded from: {self.features_path}, Shape: {features_df.shape}\\n\")\n",
    "            return features_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading features from {self.features_path}: {e}\\n\")\n",
    "            return None\n",
    "\n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate evaluation metrics: PLCC, SRCC, KRCC, RMSE.\"\"\"\n",
    "        mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "        y_true_clean, y_pred_clean = y_true[mask], y_pred[mask]\n",
    "\n",
    "        if len(y_true_clean) == 0:\n",
    "            return {'PLCC': np.nan, 'SRCC': np.nan, 'KRCC': np.nan, 'RMSE': np.nan}\n",
    "\n",
    "        plcc, _ = pearsonr(y_true_clean, y_pred_clean)\n",
    "        srcc, _ = spearmanr(y_true_clean, y_pred_clean)\n",
    "        krcc, _ = kendalltau(y_true_clean, y_pred_clean)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "\n",
    "        return {'PLCC': plcc, 'SRCC': srcc, 'KRCC': krcc, 'RMSE': rmse}\n",
    "\n",
    "    def _train_model(self, model_name, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train a single model with hyperparameter tuning using RandomizedSearchCV.\"\"\"\n",
    "        model_config = self.models[model_name]\n",
    "        model_class = model_config['model']\n",
    "        param_grid = model_config['params']\n",
    "\n",
    "        print(f\"    Training {model_name}...\")\n",
    "\n",
    "        if model_name == 'svr':\n",
    "            model = model_class()\n",
    "        else: # MLP\n",
    "            model = model_class(random_state=42, early_stopping=True, validation_fraction=0.1)\n",
    "\n",
    "        search_cv = RandomizedSearchCV(\n",
    "            model, param_grid, n_iter=20, cv=3,\n",
    "            scoring='neg_mean_squared_error', random_state=42, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        search_cv.fit(X_train, y_train)\n",
    "        best_model = search_cv.best_estimator_\n",
    "        print(f\"      Best parameters: {search_cv.best_params_}\")\n",
    "\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        metrics = self._calculate_metrics(y_test, y_pred)\n",
    "\n",
    "        return best_model, metrics\n",
    "\n",
    "    def _save_model(self, model, model_name, dataset_name, label_name):\n",
    "        \"\"\"Save a trained model to its specific directory.\"\"\"\n",
    "        model_dir_name = self.model_save_dirs[model_name]\n",
    "        model_dir = os.path.join(self.models_save_path, model_dir_name)\n",
    "\n",
    "        filename = f\"{dataset_name}_{label_name}.pkl\"\n",
    "        filepath = os.path.join(model_dir, filename)\n",
    "\n",
    "        try:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            print(f\"      Model saved: {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"      Error saving model {filepath}: {e}\")\n",
    "\n",
    "    def train_all_models(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Train all configured models on the SVD dataset for all specified labels.\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"STARTING SVD MODEL TRAINING (SVR & MLP)\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        features_df = self._load_features()\n",
    "        if features_df is None:\n",
    "            print(\"Halting training due to feature loading error.\")\n",
    "            return\n",
    "\n",
    "        dataset_name = \"modified_svd_features\"\n",
    "\n",
    "        for label in self.labels:\n",
    "            print(f\"\\n  🎯 Target label: {label}\")\n",
    "\n",
    "            if label not in self.labels_df.columns:\n",
    "                print(f\"    ❌ Label {label} not found in labels file. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            y = self.labels_df[label].values\n",
    "            X = features_df.values\n",
    "\n",
    "            if len(X) != len(y):\n",
    "                print(f\"    ❌ Dimension mismatch: Features={len(X)}, Labels={len(y)}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            mask = ~(np.isnan(y) | np.isnan(X).any(axis=1))\n",
    "            X_clean, y_clean = X[mask], y[mask]\n",
    "\n",
    "            if len(X_clean) == 0:\n",
    "                print(f\"    ❌ No valid samples after cleaning. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X_clean, y_clean, test_size=test_size, random_state=random_state\n",
    "            )\n",
    "\n",
    "            print(f\"    📈 Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "            for model_name in self.models.keys():\n",
    "                print(f\"\\n    🤖 Model: {model_name}\")\n",
    "                try:\n",
    "                    model, metrics = self._train_model(\n",
    "                        model_name, X_train, y_train, X_test, y_test\n",
    "                    )\n",
    "\n",
    "                    self._save_model(model, model_name, dataset_name, label)\n",
    "\n",
    "                    result = {\n",
    "                        'dataset_name': dataset_name,\n",
    "                        'label': label,\n",
    "                        'model': model_name,\n",
    "                        'train_samples': len(X_train),\n",
    "                        'test_samples': len(X_test),\n",
    "                        **metrics\n",
    "                    }\n",
    "                    self.results.append(result)\n",
    "\n",
    "                    print(f\"      ✅ Performance - PLCC: {metrics['PLCC']:.4f}, \"\n",
    "                          f\"SRCC: {metrics['SRCC']:.4f}, RMSE: {metrics['RMSE']:.4f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"      ❌ Training failed for {model_name}: {e}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"TRAINING COMPLETED!\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        self.save_results()\n",
    "\n",
    "    def save_results(self, filename=\"svd_training_results.csv\"):\n",
    "        \"\"\"Save all training results to a new CSV file.\"\"\"\n",
    "        if self.results:\n",
    "            results_df = pd.DataFrame(self.results)\n",
    "            filepath = os.path.join(self.models_save_path, filename)\n",
    "            results_df.to_csv(filepath, index=False)\n",
    "            print(f\"\\n📊 Results saved to: {filepath}\")\n",
    "            self.display_results_summary(results_df)\n",
    "        else:\n",
    "            print(\"❌ No results to save.\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def display_results_summary(self, results_df):\n",
    "        \"\"\"Display summary statistics of the training results.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🏆 SVD TRAINING RESULTS SUMMARY 🏆\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        metrics = ['PLCC', 'SRCC', 'KRCC', 'RMSE']\n",
    "\n",
    "        print(\"\\n🥇 BEST PERFORMING MODELS BY METRIC:\")\n",
    "        print(\"-\" * 50)\n",
    "        for metric in metrics:\n",
    "            if metric == 'RMSE':\n",
    "                best_result = results_df.loc[results_df[metric].idxmin()]\n",
    "                print(f\"\\n  📉 Best {metric} (Lower is better):\")\n",
    "            else:\n",
    "                best_result = results_df.loc[results_df[metric].idxmax()]\n",
    "                print(f\"\\n  📈 Best {metric} (Higher is better):\")\n",
    "            print(f\"      🎯 {best_result['model']} for label '{best_result['label']}'\")\n",
    "            print(f\"      🎖️ Score: {best_result[metric]:.4f}\")\n",
    "\n",
    "        print(f\"\\n\\n📊 AVERAGE PERFORMANCE BY MODEL:\")\n",
    "        print(\"-\" * 50)\n",
    "        model_avg = results_df.groupby('model')[metrics].mean()\n",
    "        for model in model_avg.index:\n",
    "            print(f\"\\n  🤖 {model.upper()}:\")\n",
    "            for metric in metrics:\n",
    "                print(f\"      {metric}: {model_avg.loc[model, metric]:.4f}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the model training pipeline.\"\"\"\n",
    "\n",
    "    # --- Paths adjusted to run from within the 'regressors' folder ---\n",
    "\n",
    "    # Go up one level (to 'objective-1') then into 'features'\n",
    "    features_file_path = '../features/cleaned/modified_svd_features.csv'\n",
    "\n",
    "    # Go up two levels (to the project root) then into 'dataset'\n",
    "    labels_file_path = \"../../dataset/cleaned/cleaned-mos.csv\"\n",
    "\n",
    "    # Save models in the current directory ('.') which is the 'regressors' folder\n",
    "    save_path = \".\"\n",
    "\n",
    "    # --- Initialize and run the trainer ---\n",
    "    trainer = RegressionModelTrainer(\n",
    "        features_path=features_file_path,\n",
    "        labels_path=labels_file_path,\n",
    "        models_save_path=save_path\n",
    "    )\n",
    "\n",
    "    trainer.train_all_models(test_size=0.2, random_state=42)\n",
    "\n",
    "    print(\"\\n✅ All SVD models have been trained and results have been saved locally!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a44b10-a41e-4b03-93b3-6cc5bfae801d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
