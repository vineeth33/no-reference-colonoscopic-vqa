{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4671df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, QuantileTransformer, Normalizer\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f2e5c22-f731-4e35-adb1-b545ee38f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVDFeatureScaler:\n",
    "    \"\"\"\n",
    "    A comprehensive class for scaling SVD features with different approaches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        \"\"\"\n",
    "        Initialize with the dataset filepath\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.df = None\n",
    "        self.scaled_datasets = {}\n",
    "\n",
    "        # Load data from CSV\n",
    "        self.load_data()\n",
    "        self.feature_groups = self.get_feature_groups()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load the CSV data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            self.df = pd.read_csv(self.filepath, skipinitialspace=True)\n",
    "            print(f\"Dataset loaded successfully. Shape: {self.df.shape}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def get_feature_groups(self):\n",
    "        \"\"\"\n",
    "        Separate features into U, S, V components and different dimensions\n",
    "        \"\"\"\n",
    "\n",
    "        if self.df is None:\n",
    "            print(\"Please load data first\")\n",
    "            return\n",
    "            \n",
    "        feature_groups = {\n",
    "            'U1': [col for col in self.df.columns if col.startswith('U1_')],\n",
    "            'S1': [col for col in self.df.columns if col.startswith('S1_')],\n",
    "            'V1': [col for col in self.df.columns if col.startswith('V1_')],\n",
    "            'U2': [col for col in self.df.columns if col.startswith('U2_')],\n",
    "            'S2': [col for col in self.df.columns if col.startswith('S2_')],\n",
    "            'V2': [col for col in self.df.columns if col.startswith('V2_')]\n",
    "        }\n",
    "        \n",
    "        # Print feature group sizes\n",
    "        for group, features in feature_groups.items():\n",
    "            print(f\"{group}: {len(features)} features\")\n",
    "            \n",
    "        return feature_groups\n",
    "    \n",
    "    def analyze_feature_distributions(self, sample_size=5):\n",
    "        \"\"\"\n",
    "        Analyze the distribution of features to understand scaling needs\n",
    "        \"\"\"\n",
    "\n",
    "        if self.df is None:\n",
    "            print(\"Please load data first\")\n",
    "            return\n",
    "            \n",
    "        feature_groups = self.feature_groups\n",
    "        \n",
    "        print(\"\\n=== Feature Distribution Analysis ===\")\n",
    "        \n",
    "        for group_name, features in feature_groups.items():\n",
    "            if len(features) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Take a sample of features for analysis\n",
    "            sample_features = features[:sample_size]\n",
    "            data = self.df[sample_features]\n",
    "            \n",
    "            print(f\"\\n{group_name} Features (sample of {len(sample_features)}):\")\n",
    "            print(f\"  Mean range: {data.mean().min():.6f} to {data.mean().max():.6f}\")\n",
    "            print(f\"  Std range: {data.std().min():.6f} to {data.std().max():.6f}\")\n",
    "            print(f\"  Min range: {data.min().min():.6f} to {data.min().max():.6f}\")\n",
    "            print(f\"  Max range: {data.max().min():.6f} to {data.max().max():.6f}\")\n",
    "            \n",
    "            # Check for zero/negative values (important for log scaling)\n",
    "            zero_count = (data == 0).sum().sum()\n",
    "            negative_count = (data < 0).sum().sum()\n",
    "            print(f\"  Zero values: {zero_count}\")\n",
    "            print(f\"  Negative values: {negative_count}\")\n",
    "            \n",
    "            # Skewness analysis\n",
    "            skewness = data.apply(lambda x: stats.skew(x)).mean()\n",
    "            print(f\"  Average skewness: {skewness:.3f}\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    def apply_scaling_all_features(self, scaler_type):\n",
    "        \"\"\"\n",
    "        Apply scaling to all U, S, V features together\n",
    "        \n",
    "        Parameters:\n",
    "        scaler_type: str - 'standard', 'minmax', 'robust', 'quantile', 'power', 'log', 'log1p'\n",
    "        \"\"\"\n",
    "\n",
    "        if self.df is None:\n",
    "            print(\"Please load data first\")\n",
    "            return None\n",
    "            \n",
    "        feature_groups = self.feature_groups\n",
    "        \n",
    "        # Combine all feature columns\n",
    "        all_features = []\n",
    "        for features in feature_groups.values():\n",
    "            all_features.extend(features)\n",
    "        \n",
    "        if len(all_features) == 0:\n",
    "            print(\"No features found\")\n",
    "            return None\n",
    "            \n",
    "        # Get the feature data\n",
    "        X = self.df[all_features].copy()\n",
    "        \n",
    "        # Apply scaling based on type\n",
    "        X_scaled = self._apply_scaler(X, scaler_type)\n",
    "        \n",
    "        if X_scaled is not None:\n",
    "            # Create scaled dataset\n",
    "            scaled_df = self.df.copy()\n",
    "            scaled_df[all_features] = X_scaled\n",
    "            \n",
    "            dataset_name = f\"all_features_{scaler_type}\"\n",
    "            self.scaled_datasets[dataset_name] = scaled_df\n",
    "            \n",
    "            print(f\"Applied {scaler_type} scaling to all features\")\n",
    "            return scaled_df\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def apply_scaling_by_component(self, scaler_type='standard'):\n",
    "        \"\"\"\n",
    "        Apply scaling to each component (U1, S1, V1, U2, S2, V2) separately\n",
    "        \n",
    "        Parameters:\n",
    "        scaler_type: str - 'standard', 'minmax', 'robust', 'quantile', 'power', 'log', 'log1p'\n",
    "        \"\"\"\n",
    "\n",
    "        if self.df is None:\n",
    "            print(\"Please load data first\")\n",
    "            return None\n",
    "            \n",
    "        feature_groups = self.feature_groups\n",
    "        scaled_df = self.df.copy()\n",
    "        \n",
    "        for group_name, features in feature_groups.items():\n",
    "            if len(features) == 0:\n",
    "                continue\n",
    "                \n",
    "            X = self.df[features].copy()\n",
    "            X_scaled = self._apply_scaler(X, scaler_type)\n",
    "            \n",
    "            if X_scaled is not None:\n",
    "                scaled_df[features] = X_scaled\n",
    "                print(f\"Applied {scaler_type} scaling to {group_name} ({len(features)} features)\")\n",
    "        \n",
    "        dataset_name = f\"by_component_{scaler_type}\"\n",
    "        self.scaled_datasets[dataset_name] = scaled_df\n",
    "        \n",
    "        return scaled_df\n",
    "    \n",
    "    def apply_scaling_s_only(self, scaler_type='standard'):\n",
    "        \"\"\"\n",
    "        Apply scaling only to S features (singular values)\n",
    "        \n",
    "        Parameters:\n",
    "        scaler_type: str - 'standard', 'minmax', 'robust', 'quantile', 'power', 'log', 'log1p'\n",
    "        \"\"\"\n",
    "\n",
    "        if self.df is None:\n",
    "            print(\"Please load data first\")\n",
    "            return None\n",
    "\n",
    "        feature_groups = self.feature_groups\n",
    "        scaled_df = self.df.copy()\n",
    "        \n",
    "        # Only scale S features\n",
    "        s_features = feature_groups['S1'] + feature_groups['S2']\n",
    "        \n",
    "        if len(s_features) == 0:\n",
    "            print(\"No S features found\")\n",
    "            return None\n",
    "            \n",
    "        X = self.df[s_features].copy()\n",
    "        X_scaled = self._apply_scaler(X, scaler_type)\n",
    "        \n",
    "        if X_scaled is not None:\n",
    "            scaled_df[s_features] = X_scaled\n",
    "            print(f\"Applied {scaler_type} scaling to S features only ({len(s_features)} features)\")\n",
    "        \n",
    "        dataset_name = f\"s_only_{scaler_type}\"\n",
    "        self.scaled_datasets[dataset_name] = scaled_df\n",
    "        \n",
    "        return scaled_df\n",
    "    \n",
    "    def _apply_scaler(self, X, scaler_type):\n",
    "        \"\"\"\n",
    "        Internal method to apply different types of scalers\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if scaler_type == 'standard':\n",
    "                scaler = StandardScaler()\n",
    "                return scaler.fit_transform(X)\n",
    "            \n",
    "            elif scaler_type == 'minmax':\n",
    "                scaler = MinMaxScaler()\n",
    "                return scaler.fit_transform(X)\n",
    "            \n",
    "            elif scaler_type == 'robust':\n",
    "                scaler = RobustScaler()\n",
    "                return scaler.fit_transform(X)\n",
    "            \n",
    "            elif scaler_type == 'quantile':\n",
    "                scaler = QuantileTransformer(output_distribution='normal')\n",
    "                return scaler.fit_transform(X)\n",
    "            \n",
    "            elif scaler_type == 'power':\n",
    "                scaler = PowerTransformer(method='yeo-johnson')\n",
    "                return scaler.fit_transform(X)\n",
    "            \n",
    "            elif scaler_type == 'log':\n",
    "                # Add small constant to handle zeros/negatives\n",
    "                X_positive = X + abs(X.min().min()) + 1e-8\n",
    "                return np.log(X_positive)\n",
    "            \n",
    "            elif scaler_type == 'log1p':\n",
    "                # Add small constant to handle negatives\n",
    "                X_positive = X + abs(X.min().min()) + 1e-8\n",
    "                return np.log1p(X_positive)\n",
    "            \n",
    "            else:\n",
    "                print(f\"Unknown scaler type: {scaler_type}\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error applying {scaler_type} scaling: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def run_comprehensive_scaling(self):\n",
    "        \"\"\"\n",
    "        Run all scaling approaches with different scalers\n",
    "        \"\"\"\n",
    "        scalers = ['standard', 'minmax', 'robust', 'quantile', 'power', 'log', 'log1p']\n",
    "        \n",
    "        print(\"=== Running Comprehensive Scaling Analysis ===\")\n",
    "        \n",
    "        for scaler in scalers:\n",
    "            print(f\"\\n--- Applying {scaler.upper()} Scaling ---\")\n",
    "            \n",
    "            # All features together\n",
    "            self.apply_scaling_all_features(scaler)\n",
    "            \n",
    "            # By component\n",
    "            self.apply_scaling_by_component(scaler)\n",
    "            \n",
    "            # S features only\n",
    "            self.apply_scaling_s_only(scaler)\n",
    "        \n",
    "        print(f\"\\nCompleted! Generated {len(self.scaled_datasets)} scaled datasets\")\n",
    "        print(\"Available datasets:\", list(self.scaled_datasets.keys()), \"\\n\\n\")\n",
    "    \n",
    "    def compare_scaling_effects(self, feature_sample_size=3):\n",
    "        \"\"\"\n",
    "        Compare the effects of different scaling approaches\n",
    "        \"\"\"\n",
    "        if len(self.scaled_datasets) == 0:\n",
    "            print(\"No scaled datasets available. Run scaling first.\")\n",
    "            return\n",
    "        \n",
    "        feature_groups = self.feature_groups\n",
    "        \n",
    "        # Sample features from each group for comparison\n",
    "        sample_features = []\n",
    "        for group_name, features in feature_groups.items():\n",
    "            if len(features) > 0:\n",
    "                sample_features.extend(features[:min(feature_sample_size, len(features))])\n",
    "        \n",
    "        print(\"=== Scaling Effects Comparison ===\")\n",
    "        \n",
    "        # Original data stats\n",
    "        original_data = self.df[sample_features]\n",
    "        print(f\"\\nOriginal Data:\")\n",
    "        print(f\"  Mean: {original_data.mean().mean():.6f} ± {original_data.mean().std():.6f}\")\n",
    "        print(f\"  Std: {original_data.std().mean():.6f} ± {original_data.std().std():.6f}\")\n",
    "        print(f\"  Range: [{original_data.min().min():.6f}, {original_data.max().max():.6f}]\")\n",
    "        \n",
    "        # Compare scaled datasets\n",
    "        for dataset_name, scaled_df in self.scaled_datasets.items():\n",
    "            scaled_data = scaled_df[sample_features]\n",
    "            print(f\"\\n{dataset_name}:\")\n",
    "            print(f\"  Mean: {scaled_data.mean().mean():.6f} ± {scaled_data.mean().std():.6f}\")\n",
    "            print(f\"  Std: {scaled_data.std().mean():.6f} ± {scaled_data.std().std():.6f}\")\n",
    "            print(f\"  Range: [{scaled_data.min().min():.6f}, {scaled_data.max().max():.6f}]\")\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "    \n",
    "    def get_scaled_dataset(self, dataset_name):\n",
    "        \"\"\"\n",
    "        Retrieve a specific scaled dataset\n",
    "        \"\"\"\n",
    "        return self.scaled_datasets.get(dataset_name, None)\n",
    "    \n",
    "    def save_scaled_dataset(self, dataset_name, filepath):\n",
    "        \"\"\"\n",
    "        Save a scaled dataset to file\n",
    "        \"\"\"\n",
    "        if dataset_name in self.scaled_datasets:\n",
    "            self.scaled_datasets[dataset_name].to_csv(filepath, index=False)\n",
    "            print(f\"Saved {dataset_name} to {filepath}\")\n",
    "        else:\n",
    "            print(f\"Dataset {dataset_name} not found\")\n",
    "\n",
    "    def save_all_datasets(self, outputDir):\n",
    "        print(\"=== Saving Scaled Datasets ===\")\n",
    "        for key in self.scaled_datasets:\n",
    "            self.save_scaled_dataset(key, f'{outputDir}/{key.split('_')[-1]}/{key}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68bb9c12-86f8-45fb-b755-28e8d245e356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully. Shape: (1000, 1153)\n",
      "U1: 256 features\n",
      "S1: 256 features\n",
      "V1: 256 features\n",
      "U2: 128 features\n",
      "S2: 128 features\n",
      "V2: 128 features\n",
      "\n",
      "=== Feature Distribution Analysis ===\n",
      "\n",
      "U1 Features (sample of 5):\n",
      "  Mean range: 0.162150 to 0.317315\n",
      "  Std range: 0.008142 to 0.022730\n",
      "  Min range: 0.144225 to 0.262484\n",
      "  Max range: 0.201038 to 0.410429\n",
      "  Zero values: 0\n",
      "  Negative values: 0\n",
      "  Average skewness: 0.821\n",
      "\n",
      "S1 Features (sample of 5):\n",
      "  Mean range: 893.854290 to 14065.820617\n",
      "  Std range: 192.640028 to 865.363380\n",
      "  Min range: 415.141994 to 9420.811306\n",
      "  Max range: 1642.627389 to 16710.755937\n",
      "  Zero values: 0\n",
      "  Negative values: 0\n",
      "  Average skewness: 0.362\n",
      "\n",
      "V1 Features (sample of 5):\n",
      "  Mean range: 0.173614 to 0.346338\n",
      "  Std range: 0.010251 to 0.026565\n",
      "  Min range: 0.148395 to 0.266828\n",
      "  Max range: 0.220958 to 0.446164\n",
      "  Zero values: 0\n",
      "  Negative values: 0\n",
      "  Average skewness: 0.371\n",
      "\n",
      "U2 Features (sample of 5):\n",
      "  Mean range: 0.228297 to 0.417702\n",
      "  Std range: 0.011417 to 0.029647\n",
      "  Min range: 0.203487 to 0.354506\n",
      "  Max range: 0.282483 to 0.552427\n",
      "  Zero values: 0\n",
      "  Negative values: 0\n",
      "  Average skewness: 0.603\n",
      "\n",
      "S2 Features (sample of 5):\n",
      "  Mean range: 831.726975 to 14017.837932\n",
      "  Std range: 188.711343 to 872.083734\n",
      "  Min range: 357.303065 to 9372.394412\n",
      "  Max range: 1558.367074 to 16696.326410\n",
      "  Zero values: 0\n",
      "  Negative values: 0\n",
      "  Average skewness: 0.323\n",
      "\n",
      "V2 Features (sample of 5):\n",
      "  Mean range: 0.244025 to 0.442678\n",
      "  Std range: 0.014380 to 0.035663\n",
      "  Min range: 0.209343 to 0.358630\n",
      "  Max range: 0.312314 to 0.584662\n",
      "  Zero values: 0\n",
      "  Negative values: 0\n",
      "  Average skewness: 0.426\n",
      "\n",
      "\n",
      "=== Running Comprehensive Scaling Analysis ===\n",
      "\n",
      "--- Applying STANDARD Scaling ---\n",
      "Applied standard scaling to all features\n",
      "Applied standard scaling to U1 (256 features)\n",
      "Applied standard scaling to S1 (256 features)\n",
      "Applied standard scaling to V1 (256 features)\n",
      "Applied standard scaling to U2 (128 features)\n",
      "Applied standard scaling to S2 (128 features)\n",
      "Applied standard scaling to V2 (128 features)\n",
      "Applied standard scaling to S features only (384 features)\n",
      "\n",
      "--- Applying MINMAX Scaling ---\n",
      "Applied minmax scaling to all features\n",
      "Applied minmax scaling to U1 (256 features)\n",
      "Applied minmax scaling to S1 (256 features)\n",
      "Applied minmax scaling to V1 (256 features)\n",
      "Applied minmax scaling to U2 (128 features)\n",
      "Applied minmax scaling to S2 (128 features)\n",
      "Applied minmax scaling to V2 (128 features)\n",
      "Applied minmax scaling to S features only (384 features)\n",
      "\n",
      "--- Applying ROBUST Scaling ---\n",
      "Applied robust scaling to all features\n",
      "Applied robust scaling to U1 (256 features)\n",
      "Applied robust scaling to S1 (256 features)\n",
      "Applied robust scaling to V1 (256 features)\n",
      "Applied robust scaling to U2 (128 features)\n",
      "Applied robust scaling to S2 (128 features)\n",
      "Applied robust scaling to V2 (128 features)\n",
      "Applied robust scaling to S features only (384 features)\n",
      "\n",
      "--- Applying QUANTILE Scaling ---\n",
      "Applied quantile scaling to all features\n",
      "Applied quantile scaling to U1 (256 features)\n",
      "Applied quantile scaling to S1 (256 features)\n",
      "Applied quantile scaling to V1 (256 features)\n",
      "Applied quantile scaling to U2 (128 features)\n",
      "Applied quantile scaling to S2 (128 features)\n",
      "Applied quantile scaling to V2 (128 features)\n",
      "Applied quantile scaling to S features only (384 features)\n",
      "\n",
      "--- Applying POWER Scaling ---\n",
      "Applied power scaling to all features\n",
      "Applied power scaling to U1 (256 features)\n",
      "Applied power scaling to S1 (256 features)\n",
      "Applied power scaling to V1 (256 features)\n",
      "Applied power scaling to U2 (128 features)\n",
      "Applied power scaling to S2 (128 features)\n",
      "Applied power scaling to V2 (128 features)\n",
      "Applied power scaling to S features only (384 features)\n",
      "\n",
      "--- Applying LOG Scaling ---\n",
      "Applied log scaling to all features\n",
      "Applied log scaling to U1 (256 features)\n",
      "Applied log scaling to S1 (256 features)\n",
      "Applied log scaling to V1 (256 features)\n",
      "Applied log scaling to U2 (128 features)\n",
      "Applied log scaling to S2 (128 features)\n",
      "Applied log scaling to V2 (128 features)\n",
      "Applied log scaling to S features only (384 features)\n",
      "\n",
      "--- Applying LOG1P Scaling ---\n",
      "Applied log1p scaling to all features\n",
      "Applied log1p scaling to U1 (256 features)\n",
      "Applied log1p scaling to S1 (256 features)\n",
      "Applied log1p scaling to V1 (256 features)\n",
      "Applied log1p scaling to U2 (128 features)\n",
      "Applied log1p scaling to S2 (128 features)\n",
      "Applied log1p scaling to V2 (128 features)\n",
      "Applied log1p scaling to S features only (384 features)\n",
      "\n",
      "Completed! Generated 21 scaled datasets\n",
      "Available datasets: ['all_features_standard', 'by_component_standard', 's_only_standard', 'all_features_minmax', 'by_component_minmax', 's_only_minmax', 'all_features_robust', 'by_component_robust', 's_only_robust', 'all_features_quantile', 'by_component_quantile', 's_only_quantile', 'all_features_power', 'by_component_power', 's_only_power', 'all_features_log', 'by_component_log', 's_only_log', 'all_features_log1p', 'by_component_log1p', 's_only_log1p'] \n",
      "\n",
      "\n",
      "=== Scaling Effects Comparison ===\n",
      "\n",
      "Original Data:\n",
      "  Mean: 2094.387123 ± 4475.725262\n",
      "  Std: 187.361038 ± 305.842354\n",
      "  Range: [0.144225, 16710.755937]\n",
      "\n",
      "all_features_standard:\n",
      "  Mean: 0.000000 ± 0.000000\n",
      "  Std: 1.000500 ± 0.000000\n",
      "  Range: [-5.370384, 5.824261]\n",
      "\n",
      "by_component_standard:\n",
      "  Mean: 0.000000 ± 0.000000\n",
      "  Std: 1.000500 ± 0.000000\n",
      "  Range: [-5.370384, 5.824261]\n",
      "\n",
      "s_only_standard:\n",
      "  Mean: 0.201877 ± 0.164196\n",
      "  Std: 0.348336 ± 0.474579\n",
      "  Range: [-5.370384, 3.632466]\n",
      "\n",
      "all_features_minmax:\n",
      "  Mean: 0.419515 ± 0.094462\n",
      "  Std: 0.138543 ± 0.015417\n",
      "  Range: [0.000000, 1.000000]\n",
      "\n",
      "by_component_minmax:\n",
      "  Mean: 0.419515 ± 0.094462\n",
      "  Std: 0.138543 ± 0.015417\n",
      "  Range: [0.000000, 1.000000]\n",
      "\n",
      "s_only_minmax:\n",
      "  Mean: 0.371340 ± 0.135081\n",
      "  Std: 0.063769 ± 0.062023\n",
      "  Range: [0.000000, 1.000000]\n",
      "\n",
      "all_features_robust:\n",
      "  Mean: 0.061104 ± 0.045853\n",
      "  Std: 0.837564 ± 0.061828\n",
      "  Range: [-4.640518, 6.153675]\n",
      "\n",
      "by_component_robust:\n",
      "  Mean: 0.061104 ± 0.045853\n",
      "  Std: 0.837564 ± 0.061828\n",
      "  Range: [-4.640518, 6.153675]\n",
      "\n",
      "s_only_robust:\n",
      "  Mean: 0.215040 ± 0.148091\n",
      "  Std: 0.288624 ± 0.388422\n",
      "  Range: [-4.640518, 3.102783]\n",
      "\n",
      "all_features_quantile:\n",
      "  Mean: 0.000000 ± 0.000000\n",
      "  Std: 1.020366 ± 0.000000\n",
      "  Range: [-5.199338, 5.199338]\n",
      "\n",
      "by_component_quantile:\n",
      "  Mean: 0.000000 ± 0.000000\n",
      "  Std: 1.020366 ± 0.000000\n",
      "  Range: [-5.199338, 5.199338]\n",
      "\n",
      "s_only_quantile:\n",
      "  Mean: 0.201877 ± 0.164196\n",
      "  Std: 0.354958 ± 0.484214\n",
      "  Range: [-5.199338, 5.199338]\n",
      "\n",
      "all_features_power:\n",
      "  Mean: 0.000000 ± 0.000000\n",
      "  Std: 1.000500 ± 0.000000\n",
      "  Range: [-4.506998, 3.740582]\n",
      "\n",
      "by_component_power:\n",
      "  Mean: 0.000000 ± 0.000000\n",
      "  Std: 1.000500 ± 0.000000\n",
      "  Range: [-4.506998, 3.740582]\n",
      "\n",
      "s_only_power:\n",
      "  Mean: 0.201877 ± 0.164196\n",
      "  Std: 0.348336 ± 0.474579\n",
      "  Range: [-4.050653, 3.616186]\n",
      "\n",
      "all_features_log:\n",
      "  Mean: 1.948259 ± 4.682015\n",
      "  Std: 0.092110 ± 0.046208\n",
      "  Range: [-1.936378, 9.723808]\n",
      "\n",
      "by_component_log:\n",
      "  Mean: 2.268549 ± 4.447474\n",
      "  Std: 0.075177 ± 0.055431\n",
      "  Range: [-1.243231, 9.723808]\n",
      "\n",
      "s_only_log:\n",
      "  Mean: 2.978931 ± 3.931263\n",
      "  Std: 0.060461 ± 0.064110\n",
      "  Range: [0.144225, 9.723808]\n",
      "\n",
      "all_features_log1p:\n",
      "  Mean: 2.951906 ± 3.950693\n",
      "  Std: 0.056684 ± 0.066216\n",
      "  Range: [0.134728, 9.723868]\n",
      "\n",
      "by_component_log1p:\n",
      "  Mean: 3.036330 ± 3.889909\n",
      "  Std: 0.055378 ± 0.067013\n",
      "  Range: [0.253441, 9.723868]\n",
      "\n",
      "s_only_log1p:\n",
      "  Mean: 2.979044 ± 3.931408\n",
      "  Std: 0.060441 ± 0.064075\n",
      "  Range: [0.144225, 9.723868]\n",
      "\n",
      "\n",
      "\n",
      "=== Saving Scaled Datasets ===\n",
      "Saved all_features_standard to ./scaled-features/standard/all_features_standard.csv\n",
      "Saved by_component_standard to ./scaled-features/standard/by_component_standard.csv\n",
      "Saved s_only_standard to ./scaled-features/standard/s_only_standard.csv\n",
      "Saved all_features_minmax to ./scaled-features/minmax/all_features_minmax.csv\n",
      "Saved by_component_minmax to ./scaled-features/minmax/by_component_minmax.csv\n",
      "Saved s_only_minmax to ./scaled-features/minmax/s_only_minmax.csv\n",
      "Saved all_features_robust to ./scaled-features/robust/all_features_robust.csv\n",
      "Saved by_component_robust to ./scaled-features/robust/by_component_robust.csv\n",
      "Saved s_only_robust to ./scaled-features/robust/s_only_robust.csv\n",
      "Saved all_features_quantile to ./scaled-features/quantile/all_features_quantile.csv\n",
      "Saved by_component_quantile to ./scaled-features/quantile/by_component_quantile.csv\n",
      "Saved s_only_quantile to ./scaled-features/quantile/s_only_quantile.csv\n",
      "Saved all_features_power to ./scaled-features/power/all_features_power.csv\n",
      "Saved by_component_power to ./scaled-features/power/by_component_power.csv\n",
      "Saved s_only_power to ./scaled-features/power/s_only_power.csv\n",
      "Saved all_features_log to ./scaled-features/log/all_features_log.csv\n",
      "Saved by_component_log to ./scaled-features/log/by_component_log.csv\n",
      "Saved s_only_log to ./scaled-features/log/s_only_log.csv\n",
      "Saved all_features_log1p to ./scaled-features/log1p/all_features_log1p.csv\n",
      "Saved by_component_log1p to ./scaled-features/log1p/by_component_log1p.csv\n",
      "Saved s_only_log1p to ./scaled-features/log1p/s_only_log1p.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Get the file path\n",
    "    input_file = './../../features/cleaned/cleaned-svd-features.csv'\n",
    "    \n",
    "    # Initialize the scaler\n",
    "    scaler = SVDFeatureScaler(input_file)\n",
    "    \n",
    "    # Analyze distributions to understand the data\n",
    "    scaler.analyze_feature_distributions()\n",
    "    \n",
    "    # Run comprehensive scaling\n",
    "    scaler.run_comprehensive_scaling()\n",
    "    \n",
    "    # Compare scaling effects\n",
    "    scaler.compare_scaling_effects()\n",
    "\n",
    "    # Save the datasets\n",
    "    scaler.save_all_datasets(\"./scaled-features\")\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8670f-be8d-48d7-981d-c470a8bb0bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
