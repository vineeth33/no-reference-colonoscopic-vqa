{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec8b33a6-8adc-4e5c-9185-be4057e6ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from itertools import combinations\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d5da6ee-1dc7-48d1-918d-679f0308c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVRegressionModelTrainer:\n",
    "    \n",
    "    def __init__(self, csv_file_path, labels_csv_path, models_save_path):\n",
    "        self.csv_file_path = csv_file_path\n",
    "        self.labels_csv_path = labels_csv_path\n",
    "        self.models_save_path = models_save_path\n",
    "        \n",
    "        # Define feature ranges based on your dataset structure\n",
    "        self.level1_features = {\n",
    "            'U1': [f'U1_{i}' for i in range(1, 257)],  # U1_1 to U1_256\n",
    "            'S1': [f'S1_{i}' for i in range(1, 257)],  # S1_1 to S1_256\n",
    "            'V1': [f'V1_{i}' for i in range(1, 257)]   # V1_1 to V1_256\n",
    "        }\n",
    "        \n",
    "        self.level2_features = {\n",
    "            'U2': [f'U2_{i}' for i in range(1, 129)],  # U2_1 to U2_128\n",
    "            'S2': [f'S2_{i}' for i in range(1, 129)],  # S2_1 to S2_128\n",
    "            'V2': [f'V2_{i}' for i in range(1, 129)]   # V2_1 to V2_128\n",
    "        }\n",
    "        \n",
    "        # Flatten all features\n",
    "        self.all_level1_features = []\n",
    "        for feature_group in self.level1_features.values():\n",
    "            self.all_level1_features.extend(feature_group)\n",
    "            \n",
    "        self.all_level2_features = []\n",
    "        for feature_group in self.level2_features.values():\n",
    "            self.all_level2_features.extend(feature_group)\n",
    "            \n",
    "        self.all_features = self.all_level1_features + self.all_level2_features\n",
    "        \n",
    "        # Target labels\n",
    "        self.labels = ['TSV', 'B', 'SR', 'S', 'U', 'O']\n",
    "        \n",
    "        # Generate feature combinations\n",
    "        self.feature_combinations = self._generate_feature_combinations()\n",
    "        \n",
    "        # Initialize models\n",
    "        self.models = self._initialize_models()\n",
    "        \n",
    "        # Create directories and load data\n",
    "        self._create_model_directories()\n",
    "        self.data_df, self.labels_df, self.merged_df = self._load_csv_data()\n",
    "        self.results = []\n",
    "    \n",
    "    def _generate_feature_combinations(self):\n",
    "        \"\"\"Generate all possible feature combinations\"\"\"\n",
    "        combinations_dict = {}\n",
    "        \n",
    "        # Level 1 combinations (all possible combinations of U1, S1, V1)\n",
    "        level1_component_names = ['U1', 'S1', 'V1']\n",
    "        \n",
    "        for r in range(1, len(level1_component_names) + 1):\n",
    "            for combo in combinations(level1_component_names, r):\n",
    "                features_list = []\n",
    "                for component in combo:\n",
    "                    features_list.extend(self.level1_features[component])\n",
    "                \n",
    "                combo_name = f\"level1_{'_'.join(combo)}\"\n",
    "                combinations_dict[combo_name] = features_list\n",
    "        \n",
    "        # Level 2 combinations (all possible combinations of U2, S2, V2)\n",
    "        level2_component_names = ['U2', 'S2', 'V2']\n",
    "        \n",
    "        for r in range(1, len(level2_component_names) + 1):\n",
    "            for combo in combinations(level2_component_names, r):\n",
    "                features_list = []\n",
    "                for component in combo:\n",
    "                    features_list.extend(self.level2_features[component])\n",
    "                \n",
    "                combo_name = f\"level2_{'_'.join(combo)}\"\n",
    "                combinations_dict[combo_name] = features_list\n",
    "        \n",
    "        # Combined Level 1 & 2 combinations\n",
    "        level1_level2_combinations = [\n",
    "            (['U1'], ['U2']),\n",
    "            (['S1'], ['S2']),\n",
    "            (['V1'], ['V2']),\n",
    "            (['U1', 'S1'], ['U2', 'S2']),\n",
    "            (['U1', 'V1'], ['U2', 'V2']),\n",
    "            (['S1', 'V1'], ['S2', 'V2']),\n",
    "            (['U1', 'S1', 'V1'], ['U2', 'S2', 'V2'])\n",
    "        ]\n",
    "        \n",
    "        for level1_combo, level2_combo in level1_level2_combinations:\n",
    "            features_list = []\n",
    "            \n",
    "            # Add level 1 features\n",
    "            for component in level1_combo:\n",
    "                features_list.extend(self.level1_features[component])\n",
    "            \n",
    "            # Add level 2 features\n",
    "            for component in level2_combo:\n",
    "                features_list.extend(self.level2_features[component])\n",
    "            \n",
    "            combo_name = f\"combined_{'_'.join(level1_combo + level2_combo)}\"\n",
    "            combinations_dict[combo_name] = features_list\n",
    "        \n",
    "        print(f\"Generated {len(combinations_dict)} feature combinations\")\n",
    "        \n",
    "        level1_count = sum(1 for name in combinations_dict.keys() if name.startswith('level1_'))\n",
    "        level2_count = sum(1 for name in combinations_dict.keys() if name.startswith('level2_'))\n",
    "        combined_count = sum(1 for name in combinations_dict.keys() if name.startswith('combined_'))\n",
    "        \n",
    "        print(f\"Level 1 combinations: {level1_count}\")\n",
    "        print(f\"Level 2 combinations: {level2_count}\")\n",
    "        print(f\"Combined L1+L2 combinations: {combined_count}\")\n",
    "        \n",
    "        return combinations_dict\n",
    "    \n",
    "    def _initialize_models(self):\n",
    "        \"\"\"Initialize all regression models with hyperparameter grids\"\"\"\n",
    "        models = {\n",
    "            'mlp_regressor': {\n",
    "                'model': MLPRegressor,\n",
    "                'params': {\n",
    "                    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100), (100, 50, 25)],\n",
    "                    'activation': ['relu', 'tanh'],\n",
    "                    'solver': ['adam', 'lbfgs'],\n",
    "                    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "                    'learning_rate': ['constant', 'adaptive'],\n",
    "                    'max_iter': [1000, 2000]\n",
    "                }\n",
    "            },\n",
    "            'ridge_regressor': {\n",
    "                'model': Ridge,\n",
    "                'params': {\n",
    "                    'alpha': [0.01, 0.1, 1, 10, 100, 1000],\n",
    "                    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sag', 'saga'],\n",
    "                    'max_iter': [1000, 2000, 3000]\n",
    "                }\n",
    "            },\n",
    "            'decision_tree_regressor': {\n",
    "                'model': DecisionTreeRegressor,\n",
    "                'params': {\n",
    "                    'max_depth': [None, 5, 10, 15, 20, 25],\n",
    "                    'min_samples_split': [2, 5, 10, 20],\n",
    "                    'min_samples_leaf': [1, 2, 4, 8],\n",
    "                    'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "                    'criterion': ['squared_error', 'friedman_mse', 'absolute_error']\n",
    "                }\n",
    "            },\n",
    "            'random_forest_regressor': {\n",
    "                'model': RandomForestRegressor,\n",
    "                'params': {\n",
    "                    'n_estimators': [50, 100, 200, 300],\n",
    "                    'max_depth': [None, 10, 20, 30],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4],\n",
    "                    'max_features': ['auto', 'sqrt', 'log2']\n",
    "                }\n",
    "            },\n",
    "            'extra_trees_regressor': {\n",
    "                'model': ExtraTreesRegressor,\n",
    "                'params': {\n",
    "                    'n_estimators': [50, 100, 200, 300],\n",
    "                    'max_depth': [None, 10, 20, 30],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4],\n",
    "                    'max_features': ['auto', 'sqrt', 'log2']\n",
    "                }\n",
    "            },\n",
    "            'gradient_boosting_regressor': {\n",
    "                'model': GradientBoostingRegressor,\n",
    "                'params': {\n",
    "                    'n_estimators': [50, 100, 200],\n",
    "                    'learning_rate': [0.01, 0.1, 0.2],\n",
    "                    'max_depth': [3, 5, 7],\n",
    "                    'min_samples_split': [2, 5, 10],\n",
    "                    'min_samples_leaf': [1, 2, 4],\n",
    "                    'subsample': [0.8, 0.9, 1.0]\n",
    "                }\n",
    "            },\n",
    "            'adaboost_regressor': {\n",
    "                'model': AdaBoostRegressor,\n",
    "                'params': {\n",
    "                    'n_estimators': [50, 100, 200, 300],\n",
    "                    'learning_rate': [0.01, 0.1, 0.5, 1.0, 2.0],\n",
    "                    'loss': ['linear', 'square', 'exponential']\n",
    "                }\n",
    "            },\n",
    "            'svr_regressor': {\n",
    "                'model': SVR,\n",
    "                'params': {\n",
    "                    'kernel': ['linear', 'rbf'],\n",
    "                    'C': [0.1, 1, 10, 100],\n",
    "                    'gamma': ['scale', 'auto'],\n",
    "                    'epsilon': [0.01, 0.1]\n",
    "                }\n",
    "            },\n",
    "            'xgboost_regressor': {\n",
    "                'model': xgb.XGBRegressor,\n",
    "                'params': {\n",
    "                    'n_estimators': [50, 100, 200],\n",
    "                    'max_depth': [3, 5, 7, 9],\n",
    "                    'learning_rate': [0.01, 0.1, 0.2],\n",
    "                    'subsample': [0.8, 0.9, 1.0],\n",
    "                    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "                    'reg_alpha': [0, 0.1, 1],\n",
    "                    'reg_lambda': [1, 1.5, 2]\n",
    "                }\n",
    "            },\n",
    "            'catboost_regressor': {\n",
    "                'model': CatBoostRegressor,\n",
    "                'params': {\n",
    "                    'iterations': [50, 100, 200],\n",
    "                    'depth': [4, 6, 8, 10],\n",
    "                    'learning_rate': [0.01, 0.1, 0.2],\n",
    "                    'l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "                    'bootstrap_type': ['Bayesian', 'Bernoulli', 'MVS']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def _create_model_directories(self):\n",
    "        \"\"\"Create directories for saving models\"\"\"\n",
    "        model_names = list(self.models.keys())\n",
    "        for model_name in model_names:\n",
    "            model_dir = os.path.join(self.models_save_path, model_name)\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    def _load_csv_data(self):\n",
    "        \"\"\"Load and validate CSV data from both features and labels files\"\"\"\n",
    "        try:\n",
    "            # Load features CSV\n",
    "            data_df = pd.read_csv(self.csv_file_path)\n",
    "            print(f\"Features CSV loaded successfully. Shape: {data_df.shape}\")\n",
    "            print(f\"Total columns in features: {len(data_df.columns)}\")\n",
    "            \n",
    "            # Load labels CSV\n",
    "            labels_df = pd.read_csv(self.labels_csv_path)\n",
    "            print(f\"Labels CSV loaded successfully. Shape: {labels_df.shape}\")\n",
    "            print(f\"Total columns in labels: {len(labels_df.columns)}\")\n",
    "            \n",
    "            # Check for required columns in features CSV\n",
    "            required_feature_columns = ['videoname'] + self.all_features\n",
    "            missing_feature_columns = [col for col in required_feature_columns if col not in data_df.columns]\n",
    "            \n",
    "            if missing_feature_columns:\n",
    "                print(f\"Warning: Missing {len(missing_feature_columns)} feature columns from expected features\")\n",
    "                if len(missing_feature_columns) <= 20:\n",
    "                    print(f\"Examples: {missing_feature_columns[:20]}\")\n",
    "                else:\n",
    "                    print(f\"First 20 missing feature columns: {missing_feature_columns[:20]}...\")\n",
    "            else:\n",
    "                print(\"All expected feature columns found in features dataset\")\n",
    "            \n",
    "            # Check for required columns in labels CSV\n",
    "            required_label_columns = ['videoname'] + self.labels\n",
    "            missing_label_columns = [col for col in required_label_columns if col not in labels_df.columns]\n",
    "            \n",
    "            if missing_label_columns:\n",
    "                print(f\"Warning: Missing {len(missing_label_columns)} label columns from expected labels\")\n",
    "                print(f\"Missing label columns: {missing_label_columns}\")\n",
    "            else:\n",
    "                print(\"All expected label columns found in labels dataset\")\n",
    "            \n",
    "            # Check if videoname column exists in both datasets\n",
    "            if 'videoname' not in data_df.columns:\n",
    "                print(\"Error: 'videoname' column not found in features CSV\")\n",
    "                return None, None, None\n",
    "            \n",
    "            if 'videoname' not in labels_df.columns:\n",
    "                print(\"Error: 'videoname' column not found in labels CSV\")\n",
    "                return None, None, None\n",
    "            \n",
    "            # Merge datasets on videoname\n",
    "            print(\"Merging features and labels datasets...\")\n",
    "            merged_df = pd.merge(data_df, labels_df, on='videoname', how='inner')\n",
    "            print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "            print(f\"Successfully merged {len(merged_df)} samples\")\n",
    "            \n",
    "            # Check how many samples were lost during merge\n",
    "            original_features_samples = len(data_df)\n",
    "            original_labels_samples = len(labels_df)\n",
    "            merged_samples = len(merged_df)\n",
    "            \n",
    "            print(f\"Merge statistics:\")\n",
    "            print(f\"  - Original features samples: {original_features_samples}\")\n",
    "            print(f\"  - Original labels samples: {original_labels_samples}\")\n",
    "            print(f\"  - Merged samples: {merged_samples}\")\n",
    "            print(f\"  - Samples lost: {max(original_features_samples, original_labels_samples) - merged_samples}\")\n",
    "            \n",
    "            # Display basic statistics\n",
    "            print(f\"\\nMerged dataset info:\")\n",
    "            print(f\"  - Rows: {len(merged_df)}\")\n",
    "            print(f\"  - Features available: {sum(1 for col in merged_df.columns if col in self.all_features)}/{len(self.all_features)}\")\n",
    "            print(f\"  - Labels available: {sum(1 for col in merged_df.columns if col in self.labels)}/{len(self.labels)}\")\n",
    "            \n",
    "            # Check for duplicate videonames\n",
    "            duplicate_features = data_df['videoname'].duplicated().sum()\n",
    "            duplicate_labels = labels_df['videoname'].duplicated().sum()\n",
    "            \n",
    "            if duplicate_features > 0:\n",
    "                print(f\"Warning: {duplicate_features} duplicate videonames found in features CSV\")\n",
    "            if duplicate_labels > 0:\n",
    "                print(f\"Warning: {duplicate_labels} duplicate videonames found in labels CSV\")\n",
    "            \n",
    "            return data_df, labels_df, merged_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV files: {e}\")\n",
    "            return None, None, None\n",
    "    \n",
    "    def _model_exists(self, model_name, feature_combo_name, label_name):\n",
    "        \"\"\"Check if a trained model already exists\"\"\"\n",
    "        model_dir = os.path.join(self.models_save_path, model_name)\n",
    "        filename = f\"{feature_combo_name}_{label_name}.pkl\"\n",
    "        filepath = os.path.join(model_dir, filename)\n",
    "        return os.path.exists(filepath)\n",
    "    \n",
    "    def _load_existing_model(self, model_name, feature_combo_name, label_name):\n",
    "        \"\"\"Load an existing trained model\"\"\"\n",
    "        model_dir = os.path.join(self.models_save_path, model_name)\n",
    "        filename = f\"{feature_combo_name}_{label_name}.pkl\"\n",
    "        filepath = os.path.join(model_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            print(f\"      Loaded existing model: {filename}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"      Error loading existing model {filename}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate performance metrics\"\"\"\n",
    "        # Remove NaN values\n",
    "        mask = ~(np.isnan(y_true) | np.isnan(y_pred))\n",
    "        y_true_clean = y_true[mask]\n",
    "        y_pred_clean = y_pred[mask]\n",
    "        \n",
    "        if len(y_true_clean) == 0:\n",
    "            return {'PLCC': np.nan, 'SRCC': np.nan, 'KRCC': np.nan, 'RMSE': np.nan}\n",
    "        \n",
    "        # Calculate correlation metrics\n",
    "        plcc, _ = pearsonr(y_true_clean, y_pred_clean)\n",
    "        srcc, _ = spearmanr(y_true_clean, y_pred_clean)\n",
    "        krcc, _ = kendalltau(y_true_clean, y_pred_clean)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_clean, y_pred_clean))\n",
    "        \n",
    "        return {\n",
    "            'PLCC': plcc,\n",
    "            'SRCC': srcc,\n",
    "            'KRCC': krcc,\n",
    "            'RMSE': rmse\n",
    "        }\n",
    "    \n",
    "    def _train_model(self, model_name, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Train a single model with hyperparameter tuning\"\"\"\n",
    "        model_config = self.models[model_name]\n",
    "        model_class = model_config['model']\n",
    "        param_grid = model_config['params']\n",
    "        \n",
    "        print(f\"      Training {model_name}...\")\n",
    "        \n",
    "        # Initialize model with appropriate parameters\n",
    "        if model_name == 'mlp_regressor':\n",
    "            model = model_class(random_state=42, early_stopping=True, validation_fraction=0.1)\n",
    "        elif model_name == 'xgboost_regressor':\n",
    "            model = model_class(random_state=42, objective='reg:squarederror', tree_method=\"gpu_hist\", predictor=\"gpu_predictor\")\n",
    "        elif model_name == 'catboost_regressor':\n",
    "            model = model_class(random_state=42, verbose=False, task_type=\"GPU\")\n",
    "        elif model_name == 'svr_regressor':\n",
    "            model = model_class()\n",
    "        else:\n",
    "            model = model_class(random_state=42)\n",
    "        \n",
    "        # Perform hyperparameter search\n",
    "        print(f\"      Performing hyperparameter search...\")\n",
    "        n_iter = 15 if model_name in ['xgboost_regressor', 'catboost_regressor', 'mlp_regressor', 'svr_regressor'] else 20\n",
    "        \n",
    "        search_cv = RandomizedSearchCV(\n",
    "            model, param_grid, n_iter=n_iter, cv=3, \n",
    "            scoring='neg_mean_squared_error', random_state=42, n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        search_cv.fit(X_train, y_train)\n",
    "        best_model = search_cv.best_estimator_\n",
    "        \n",
    "        # Evaluate model\n",
    "        print(f\"      Evaluating model performance...\")\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        metrics = self._calculate_metrics(y_test, y_pred)\n",
    "        \n",
    "        return best_model, metrics\n",
    "    \n",
    "    def _save_model(self, model, model_name, feature_combo_name, label_name):\n",
    "        \"\"\"Save trained model to disk\"\"\"\n",
    "        model_dir = os.path.join(self.models_save_path, model_name)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        filename = f\"{feature_combo_name}_{label_name}.pkl\"\n",
    "        filepath = os.path.join(model_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            print(f\"      Model saved: {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"      Error saving model {filename}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _save_intermediate_results(self):\n",
    "        \"\"\"Save intermediate results during training\"\"\"\n",
    "        if self.results:\n",
    "            results_df = pd.DataFrame(self.results)\n",
    "            timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"intermediate_results_{timestamp}.csv\"\n",
    "            filepath = os.path.join(self.models_save_path, filename)\n",
    "            \n",
    "            try:\n",
    "                results_df.to_csv(filepath, index=False)\n",
    "                print(f\"      Intermediate results saved: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"      Error saving intermediate results: {e}\")\n",
    "    \n",
    "    def train_all_models(self, test_size=0.2, random_state=42, force_retrain=False):\n",
    "        \"\"\"Train all models on all feature combinations and labels\"\"\"\n",
    "        if self.merged_df is None:\n",
    "            print(\"No merged data available. Cannot proceed with training.\")\n",
    "            return\n",
    "        \n",
    "        print(\"=\" * 100)\n",
    "        print(\"STARTING COMPREHENSIVE MODEL TRAINING\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        # Display training overview\n",
    "        level1_count = sum(1 for name in self.feature_combinations.keys() if name.startswith('level1_'))\n",
    "        level2_count = sum(1 for name in self.feature_combinations.keys() if name.startswith('level2_'))\n",
    "        combined_count = sum(1 for name in self.feature_combinations.keys() if name.startswith('combined_'))\n",
    "        \n",
    "        print(f\"Feature combinations breakdown:\")\n",
    "        print(f\"  - Level 1: {level1_count}\")\n",
    "        print(f\"  - Level 2: {level2_count}\")\n",
    "        print(f\"  - Combined: {combined_count}\")\n",
    "        print(f\"  - Total: {len(self.feature_combinations)}\")\n",
    "        print(f\"\\nTotal models to train: {len(self.feature_combinations)} × {len(self.labels)} × {len(self.models)} = \"\n",
    "              f\"{len(self.feature_combinations) * len(self.labels) * len(self.models)} models\\n\")\n",
    "        \n",
    "        total_models = 0\n",
    "        successful_models = 0\n",
    "        skipped_models = 0\n",
    "        \n",
    "        # Train models for each feature combination\n",
    "        for combo_idx, (feature_combo_name, feature_list) in enumerate(self.feature_combinations.items(), 1):\n",
    "            print(f\"Processing feature combination {combo_idx}/{len(self.feature_combinations)}: {feature_combo_name}\")\n",
    "            print(f\"   Features: {len(feature_list)} features\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Check if all features exist in the merged dataset\n",
    "            missing_features = [f for f in feature_list if f not in self.merged_df.columns]\n",
    "            if missing_features:\n",
    "                print(f\"    Missing features: {len(missing_features)} features not found. Skipping this combination.\\n\")\n",
    "                if len(missing_features) <= 10:\n",
    "                    print(f\"       Examples: {missing_features[:10]}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract features\n",
    "            X = self.merged_df[feature_list].values\n",
    "            \n",
    "            # Train models for each label\n",
    "            for label in self.labels:\n",
    "                print(f\"\\n  Target label: {label}\")\n",
    "                \n",
    "                if label not in self.merged_df.columns:\n",
    "                    print(f\"    Label {label} not found in merged dataset\")\n",
    "                    continue\n",
    "                \n",
    "                y = self.merged_df[label].values\n",
    "                \n",
    "                # Validate dimensions\n",
    "                if len(X) != len(y):\n",
    "                    print(f\"    Dimension mismatch: Features={len(X)}, Labels={len(y)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Clean data (remove NaN values)\n",
    "                mask = ~(np.isnan(y) | np.isnan(X).any(axis=1))\n",
    "                X_clean = X[mask]\n",
    "                y_clean = y[mask]\n",
    "                \n",
    "                if len(X_clean) == 0:\n",
    "                    print(f\"    No valid samples after cleaning\")\n",
    "                    continue\n",
    "                \n",
    "                # Split data\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X_clean, y_clean, test_size=test_size, random_state=random_state\n",
    "                )\n",
    "                \n",
    "                print(f\"    Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "                \n",
    "                # Train each model\n",
    "                for model_name in self.models.keys():\n",
    "                    total_models += 1\n",
    "                    print(f\"\\n    Model: {model_name}\")\n",
    "                    \n",
    "                    # Check if model already exists\n",
    "                    if not force_retrain and self._model_exists(model_name, feature_combo_name, label):\n",
    "                        print(f\"      Model already exists, loading...\")\n",
    "                        \n",
    "                        try:\n",
    "                            existing_model = self._load_existing_model(model_name, feature_combo_name, label)\n",
    "                            if existing_model is not None:\n",
    "                                # Evaluate existing model\n",
    "                                y_pred = existing_model.predict(X_test)\n",
    "                                metrics = self._calculate_metrics(y_test, y_pred)\n",
    "                                \n",
    "                                # Store results\n",
    "                                result = {\n",
    "                                    'feature_combination': feature_combo_name,\n",
    "                                    'features': ', '.join(feature_list[:5]) + '...' if len(feature_list) > 5 else ', '.join(feature_list),\n",
    "                                    'feature_count': len(feature_list),\n",
    "                                    'label': label,\n",
    "                                    'model': model_name,\n",
    "                                    'train_samples': len(X_train),\n",
    "                                    'test_samples': len(X_test),\n",
    "                                    **metrics\n",
    "                                }\n",
    "                                self.results.append(result)\n",
    "                                \n",
    "                                print(f\"      Performance - PLCC: {metrics['PLCC']:.4f}, \"\n",
    "                                      f\"SRCC: {metrics['SRCC']:.4f}, KRCC: {metrics['KRCC']:.4f}, \"\n",
    "                                      f\"RMSE: {metrics['RMSE']:.4f}\")\n",
    "                                \n",
    "                                successful_models += 1\n",
    "                                skipped_models += 1\n",
    "                                continue\n",
    "                        except Exception as e:\n",
    "                            print(f\"      Error with existing model: {e}, will retrain\")\n",
    "                    \n",
    "                    # Train new model\n",
    "                    try:\n",
    "                        model, metrics = self._train_model(\n",
    "                            model_name, X_train, y_train, X_test, y_test\n",
    "                        )\n",
    "                        \n",
    "                        # Save model\n",
    "                        print(f\"      Saving model...\")\n",
    "                        self._save_model(model, model_name, feature_combo_name, label)\n",
    "                        \n",
    "                        # Store results\n",
    "                        result = {\n",
    "                            'feature_combination': feature_combo_name,\n",
    "                            'features': ', '.join(feature_list[:5]) + '...' if len(feature_list) > 5 else ', '.join(feature_list),\n",
    "                            'feature_count': len(feature_list),\n",
    "                            'label': label,\n",
    "                            'model': model_name,\n",
    "                            'train_samples': len(X_train),\n",
    "                            'test_samples': len(X_test),\n",
    "                            **metrics\n",
    "                        }\n",
    "                        self.results.append(result)\n",
    "                        \n",
    "                        successful_models += 1\n",
    "                        \n",
    "                        print(f\"      Performance - PLCC: {metrics['PLCC']:.4f}, \"\n",
    "                              f\"SRCC: {metrics['SRCC']:.4f}, KRCC: {metrics['KRCC']:.4f}, \"\n",
    "                              f\"RMSE: {metrics['RMSE']:.4f}\")\n",
    "                        \n",
    "                        # Save intermediate results every 50 models\n",
    "                        if len(self.results) % 50 == 0:\n",
    "                            self._save_intermediate_results()\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"      Training failed: {e}\")\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Completed feature combination: {feature_combo_name}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Training summary\n",
    "        print(\"=\" * 100)\n",
    "        print(\"TRAINING COMPLETED!\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Successfully trained/loaded: {successful_models}/{total_models} models\")\n",
    "        print(f\"Skipped (already existed): {skipped_models}/{total_models} models\")\n",
    "        print(f\"Newly trained: {successful_models - skipped_models}/{total_models} models\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        # Save final results\n",
    "        self.save_results()\n",
    "    \n",
    "    def save_results(self, filename=\"csv_training_results.csv\"):\n",
    "        \"\"\"Save training results to CSV file\"\"\"\n",
    "        if self.results:\n",
    "            results_df = pd.DataFrame(self.results)\n",
    "            filepath = os.path.join(self.models_save_path, filename)\n",
    "            results_df.to_csv(filepath, index=False)\n",
    "            print(f\"\\nResults saved to: {filepath}\")\n",
    "            \n",
    "            # Display summary\n",
    "            self.display_results_summary(results_df)\n",
    "        else:\n",
    "            print(\"No results to save.\")\n",
    "\n",
    "        print(\"\\nTraining process completed successfully!\")\n",
    "    \n",
    "    def display_results_summary(self, results_df):\n",
    "        \"\"\"Display comprehensive results summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"TRAINING RESULTS SUMMARY\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        metrics = ['PLCC', 'SRCC', 'KRCC', 'RMSE']\n",
    "        \n",
    "        print(\"\\nBEST PERFORMING MODELS BY METRIC:\")\n",
    "        print(\"-\" * 60)\n",
    "        for metric in metrics:\n",
    "            if metric == 'RMSE':\n",
    "                best_result = results_df.loc[results_df[metric].idxmin()]\n",
    "                print(f\"\\n  Best {metric} (Lower is better):\")\n",
    "            else:\n",
    "                best_result = results_df.loc[results_df[metric].idxmax()]\n",
    "                print(f\"\\n  Best {metric} (Higher is better):\")\n",
    "                \n",
    "            print(f\"     Model: {best_result['model']}\")\n",
    "            print(f\"     Features: {best_result['feature_combination']}\")\n",
    "            print(f\"     Label: {best_result['label']}\")\n",
    "            print(f\"     Score: {best_result[metric]:.4f}\")\n",
    "        \n",
    "        print(f\"\\n\\nAVERAGE PERFORMANCE BY MODEL:\")\n",
    "        print(\"-\" * 60)\n",
    "        model_avg = results_df.groupby('model')[metrics].mean()\n",
    "        for model in model_avg.index:\n",
    "            print(f\"\\n  {model.upper().replace('_', ' ')}:\")\n",
    "            for metric in metrics:\n",
    "                print(f\"     {metric}: {model_avg.loc[model, metric]:.4f}\")\n",
    "        \n",
    "        print(f\"\\n\\nTOP 10 FEATURE COMBINATIONS (by PLCC):\")\n",
    "        print(\"-\" * 60)\n",
    "        combo_avg = results_df.groupby(['feature_combination'])['PLCC'].mean().sort_values(ascending=False).head(10)\n",
    "        for i, (combo, plcc) in enumerate(combo_avg.items(), 1):\n",
    "            print(f\"  #{i} {combo}: {plcc:.4f}\")\n",
    "        \n",
    "        print(f\"\\n\\nBEST PERFORMANCE BY LABEL:\")\n",
    "        print(\"-\" * 60)\n",
    "        for label in self.labels:\n",
    "            label_data = results_df[results_df['label'] == label]\n",
    "            if not label_data.empty:\n",
    "                best_idx = label_data['PLCC'].idxmax()\n",
    "                best_result = label_data.loc[best_idx]\n",
    "                print(f\"\\n  {label}:\")\n",
    "                print(f\"     Best Model: {best_result['model']}\")\n",
    "                print(f\"     Features: {best_result['feature_combination']}\")\n",
    "                print(f\"     PLCC: {best_result['PLCC']:.4f}\")\n",
    "                print(f\"     SRCC: {best_result['SRCC']:.4f}\")\n",
    "                print(f\"     KRCC: {best_result['KRCC']:.4f}\")\n",
    "                print(f\"     RMSE: {best_result['RMSE']:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "    \n",
    "    def load_model_for_prediction(self, model_name, feature_combo_name, label_name):\n",
    "        \"\"\"Load a specific trained model for prediction\"\"\"\n",
    "        return self._load_existing_model(model_name, feature_combo_name, label_name)\n",
    "    \n",
    "    def predict_with_model(self, model_name, feature_combo_name, label_name, X_new):\n",
    "        \"\"\"Make predictions using a trained model\"\"\"\n",
    "        model = self.load_model_for_prediction(model_name, feature_combo_name, label_name)\n",
    "        if model is not None:\n",
    "            return model.predict(X_new)\n",
    "        else:\n",
    "            print(f\"Model not found: {model_name}_{feature_combo_name}_{label_name}\")\n",
    "            return None\n",
    "    \n",
    "    def get_model_summary(self):\n",
    "        \"\"\"Get summary of available trained models\"\"\"\n",
    "        summary = []\n",
    "        for model_name in self.models.keys():\n",
    "            model_dir = os.path.join(self.models_save_path, model_name)\n",
    "            if os.path.exists(model_dir):\n",
    "                model_files = [f for f in os.listdir(model_dir) if f.endswith('.pkl')]\n",
    "                for file in model_files:\n",
    "                    parts = file.replace('.pkl', '').split('_')\n",
    "                    if len(parts) >= 2:\n",
    "                        feature_combo = '_'.join(parts[:-1])\n",
    "                        label = parts[-1]\n",
    "                        summary.append({\n",
    "                            'model': model_name,\n",
    "                            'feature_combination': feature_combo,\n",
    "                            'label': label,\n",
    "                            'file_path': os.path.join(model_dir, file)\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(summary)\n",
    "\n",
    "    def export_best_models_summary(self, top_n=10, filename=\"best_models_summary.csv\"):\n",
    "        \"\"\"Export summary of top performing models for each metric and label\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results available to export.\")\n",
    "            return\n",
    "        \n",
    "        results_df = pd.DataFrame(self.results)\n",
    "        best_models_data = []\n",
    "        \n",
    "        metrics = ['PLCC', 'SRCC', 'KRCC', 'RMSE']\n",
    "        \n",
    "        for label in self.labels:\n",
    "            label_data = results_df[results_df['label'] == label]\n",
    "            if label_data.empty:\n",
    "                continue\n",
    "                \n",
    "            for metric in metrics:\n",
    "                # Get top N models for this metric and label\n",
    "                if metric == 'RMSE':\n",
    "                    top_models = label_data.nsmallest(top_n, metric)\n",
    "                else:\n",
    "                    top_models = label_data.nlargest(top_n, metric)\n",
    "                \n",
    "                for rank, (_, row) in enumerate(top_models.iterrows(), 1):\n",
    "                    best_models_data.append({\n",
    "                        'label': label,\n",
    "                        'metric': metric,\n",
    "                        'rank': rank,\n",
    "                        'model': row['model'],\n",
    "                        'feature_combination': row['feature_combination'],\n",
    "                        'score': row[metric],\n",
    "                        'plcc': row['PLCC'],\n",
    "                        'srcc': row['SRCC'],\n",
    "                        'krcc': row['KRCC'],\n",
    "                        'rmse': row['RMSE'],\n",
    "                        'feature_count': row['feature_count']\n",
    "                    })\n",
    "        \n",
    "        best_models_df = pd.DataFrame(best_models_data)\n",
    "        filepath = os.path.join(self.models_save_path, filename)\n",
    "        best_models_df.to_csv(filepath, index=False)\n",
    "        print(f\"Best models summary exported to: {filepath}\")\n",
    "        \n",
    "        return best_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ad7d821-b857-4fef-a1e7-826f8aed0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified usage function with separate labels CSV\n",
    "def run_training(csv_file_path, labels_csv_path, models_save_path, test_size=0.2, force_retrain=False):\n",
    "    \"\"\"\n",
    "    Simple function to run the complete training process with separate features and labels files\n",
    "    \n",
    "    Args:\n",
    "        csv_file_path: Path to CSV file with features data\n",
    "        labels_csv_path: Path to CSV file with labels data\n",
    "        models_save_path: Directory to save/load models\n",
    "        test_size: Fraction for test set (default: 0.2)\n",
    "        force_retrain: Whether to retrain existing models (default: False)\n",
    "    \"\"\"\n",
    "    print(\"CSV Regression Model Trainer\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Validate CSV files exist\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        print(f\"Features CSV file not found: {csv_file_path}\")\n",
    "        return None\n",
    "    \n",
    "    if not os.path.exists(labels_csv_path):\n",
    "        print(f\"Labels CSV file not found: {labels_csv_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Features CSV file: {csv_file_path}\")\n",
    "    print(f\"Labels CSV file: {labels_csv_path}\")\n",
    "    print(f\"Models directory: {models_save_path}\")\n",
    "    print(f\"Test size: {test_size}\")\n",
    "    print(f\"Force retrain: {force_retrain}\")\n",
    "    \n",
    "    # Create trainer instance\n",
    "    trainer = CSVRegressionModelTrainer(\n",
    "        csv_file_path=csv_file_path,\n",
    "        labels_csv_path=labels_csv_path,\n",
    "        models_save_path=models_save_path\n",
    "    )\n",
    "    \n",
    "    if trainer.merged_df is None:\n",
    "        print(\"Failed to load and merge data. Exiting.\")\n",
    "        return None\n",
    "    \n",
    "    # Start training\n",
    "    print(\"\\nStarting model training...\")\n",
    "    trainer.train_all_models(\n",
    "        test_size=test_size,\n",
    "        random_state=42,\n",
    "        force_retrain=force_retrain\n",
    "    )\n",
    "    \n",
    "    # Export best models summary\n",
    "    if trainer.results:\n",
    "        print(\"\\nExporting best models summary...\")\n",
    "        trainer.export_best_models_summary(top_n=10)\n",
    "    \n",
    "    # Display model summary\n",
    "    print(\"\\nGetting model summary...\")\n",
    "    model_summary = trainer.get_model_summary()\n",
    "    if not model_summary.empty:\n",
    "        print(f\"Total trained models: {len(model_summary)}\")\n",
    "        print(\"\\nModel distribution:\")\n",
    "        print(model_summary.groupby('model').size().to_string())\n",
    "        \n",
    "        print(\"\\nLabel distribution:\")\n",
    "        print(model_summary.groupby('label').size().to_string())\n",
    "    else:\n",
    "        print(\"No models found.\")\n",
    "    \n",
    "    print(\"\\nAll processes completed successfully!\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. Check the results CSV file for performance analysis\")\n",
    "    print(\"2. Use the best_models_summary.csv to identify top performers\")\n",
    "    print(\"3. Load specific models using trainer.load_model_for_prediction() for predictions\")\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9793ef90-9888-46ba-824c-57ee0f528ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Regression Model Trainer\n",
      "==================================================\n",
      "Features CSV file: ../preprocessing/scaling/scaled-features/power/all_features_power.csv\n",
      "Labels CSV file: ../../dataset/cleaned/cleaned-mos.csv\n",
      "Models directory: ./trained_models\n",
      "Test size: 0.2\n",
      "Force retrain: False\n",
      "Generated 21 feature combinations\n",
      "Level 1 combinations: 7\n",
      "Level 2 combinations: 7\n",
      "Combined L1+L2 combinations: 7\n",
      "Features CSV loaded successfully. Shape: (1000, 1153)\n",
      "Total columns in features: 1153\n",
      "Labels CSV loaded successfully. Shape: (1000, 7)\n",
      "Total columns in labels: 7\n",
      "All expected feature columns found in features dataset\n",
      "All expected label columns found in labels dataset\n",
      "Merging features and labels datasets...\n",
      "Merged dataset shape: (1000, 1159)\n",
      "Successfully merged 1000 samples\n",
      "Merge statistics:\n",
      "  - Original features samples: 1000\n",
      "  - Original labels samples: 1000\n",
      "  - Merged samples: 1000\n",
      "  - Samples lost: 0\n",
      "\n",
      "Merged dataset info:\n",
      "  - Rows: 1000\n",
      "  - Features available: 1152/1152\n",
      "  - Labels available: 6/6\n",
      "\n",
      "Starting model training...\n",
      "====================================================================================================\n",
      "STARTING COMPREHENSIVE MODEL TRAINING\n",
      "====================================================================================================\n",
      "Feature combinations breakdown:\n",
      "  - Level 1: 7\n",
      "  - Level 2: 7\n",
      "  - Combined: 7\n",
      "  - Total: 21\n",
      "\n",
      "Total models to train: 21 × 6 × 10 = 1260 models\n",
      "\n",
      "Processing feature combination 1/21: level1_U1\n",
      "   Features: 256 features\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  Target label: TSV\n",
      "    Training samples: 800, Test samples: 200\n",
      "\n",
      "    Model: mlp_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_TSV.pkl\n",
      "      Performance - PLCC: 0.6986, SRCC: 0.6805, KRCC: 0.4891, RMSE: 0.5020\n",
      "\n",
      "    Model: ridge_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_TSV.pkl\n",
      "      Performance - PLCC: 0.7230, SRCC: 0.7300, KRCC: 0.5399, RMSE: 0.4675\n",
      "\n",
      "    Model: decision_tree_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_TSV.pkl\n",
      "      Performance - PLCC: 0.5424, SRCC: 0.4862, KRCC: 0.3500, RMSE: 0.5851\n",
      "\n",
      "    Model: random_forest_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_TSV.pkl\n",
      "      Performance - PLCC: 0.7490, SRCC: 0.7007, KRCC: 0.5195, RMSE: 0.4460\n",
      "\n",
      "    Model: extra_trees_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_TSV.pkl\n",
      "      Performance - PLCC: 0.7351, SRCC: 0.6823, KRCC: 0.5028, RMSE: 0.4555\n",
      "\n",
      "    Model: gradient_boosting_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_TSV.pkl\n",
      "      Performance - PLCC: 0.7779, SRCC: 0.7357, KRCC: 0.5510, RMSE: 0.4166\n",
      "\n",
      "    Model: adaboost_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_TSV.pkl\n",
      "      Performance - PLCC: 0.7774, SRCC: 0.7414, KRCC: 0.5581, RMSE: 0.4213\n",
      "\n",
      "    Model: svr_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_TSV.pkl\n",
      "      Performance - PLCC: 0.7773, SRCC: 0.7416, KRCC: 0.5469, RMSE: 0.4197\n",
      "\n",
      "    Model: xgboost_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_TSV.pkl\n",
      "      Performance - PLCC: 0.7645, SRCC: 0.7172, KRCC: 0.5316, RMSE: 0.4279\n",
      "\n",
      "    Model: catboost_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_TSV.pkl\n",
      "      Performance - PLCC: 0.7885, SRCC: 0.7533, KRCC: 0.5647, RMSE: 0.4083\n",
      "\n",
      "  Target label: B\n",
      "    Training samples: 800, Test samples: 200\n",
      "\n",
      "    Model: mlp_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_B.pkl\n",
      "      Performance - PLCC: 0.6086, SRCC: 0.5865, KRCC: 0.4174, RMSE: 0.6698\n",
      "\n",
      "    Model: ridge_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_B.pkl\n",
      "      Performance - PLCC: 0.6011, SRCC: 0.5614, KRCC: 0.3934, RMSE: 0.6514\n",
      "\n",
      "    Model: decision_tree_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_B.pkl\n",
      "      Performance - PLCC: 0.4149, SRCC: 0.3886, KRCC: 0.2685, RMSE: 0.7868\n",
      "\n",
      "    Model: random_forest_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_B.pkl\n",
      "      Performance - PLCC: 0.5893, SRCC: 0.5408, KRCC: 0.3785, RMSE: 0.6644\n",
      "\n",
      "    Model: extra_trees_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_B.pkl\n",
      "      Performance - PLCC: 0.5927, SRCC: 0.5409, KRCC: 0.3763, RMSE: 0.6637\n",
      "\n",
      "    Model: gradient_boosting_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_B.pkl\n",
      "      Performance - PLCC: 0.6186, SRCC: 0.5710, KRCC: 0.4117, RMSE: 0.6374\n",
      "\n",
      "    Model: adaboost_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_B.pkl\n",
      "      Performance - PLCC: 0.5902, SRCC: 0.5647, KRCC: 0.4044, RMSE: 0.6625\n",
      "\n",
      "    Model: svr_regressor\n",
      "      Training svr_regressor...\n",
      "      Performing hyperparameter search...\n",
      "      Evaluating model performance...\n",
      "      Saving model...\n",
      "      Model saved: level1_U1_B.pkl\n",
      "      Performance - PLCC: 0.6683, SRCC: 0.6291, KRCC: 0.4524, RMSE: 0.6046\n",
      "\n",
      "    Model: xgboost_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_B.pkl\n",
      "      Performance - PLCC: 0.6265, SRCC: 0.5969, KRCC: 0.4357, RMSE: 0.6321\n",
      "\n",
      "    Model: catboost_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_B.pkl\n",
      "      Performance - PLCC: 0.6356, SRCC: 0.6059, KRCC: 0.4338, RMSE: 0.6259\n",
      "\n",
      "  Target label: SR\n",
      "    Training samples: 800, Test samples: 200\n",
      "\n",
      "    Model: mlp_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_SR.pkl\n",
      "      Performance - PLCC: 0.5489, SRCC: 0.5240, KRCC: 0.3690, RMSE: 0.6313\n",
      "\n",
      "    Model: ridge_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_SR.pkl\n",
      "      Performance - PLCC: 0.5820, SRCC: 0.5835, KRCC: 0.4135, RMSE: 0.5584\n",
      "\n",
      "    Model: decision_tree_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_SR.pkl\n",
      "      Performance - PLCC: 0.3977, SRCC: 0.3728, KRCC: 0.2581, RMSE: 0.6874\n",
      "\n",
      "    Model: random_forest_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_SR.pkl\n",
      "      Performance - PLCC: 0.6535, SRCC: 0.6439, KRCC: 0.4609, RMSE: 0.5467\n",
      "\n",
      "    Model: extra_trees_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_SR.pkl\n",
      "      Performance - PLCC: 0.6644, SRCC: 0.6635, KRCC: 0.4819, RMSE: 0.5497\n",
      "\n",
      "    Model: gradient_boosting_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_SR.pkl\n",
      "      Performance - PLCC: 0.6331, SRCC: 0.6175, KRCC: 0.4408, RMSE: 0.5291\n",
      "\n",
      "    Model: adaboost_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_SR.pkl\n",
      "      Performance - PLCC: 0.6273, SRCC: 0.6238, KRCC: 0.4522, RMSE: 0.5336\n",
      "\n",
      "    Model: svr_regressor\n",
      "      Training svr_regressor...\n",
      "      Performing hyperparameter search...\n",
      "      Evaluating model performance...\n",
      "      Saving model...\n",
      "      Model saved: level1_U1_SR.pkl\n",
      "      Performance - PLCC: 0.6623, SRCC: 0.6545, KRCC: 0.4706, RMSE: 0.5097\n",
      "\n",
      "    Model: xgboost_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_SR.pkl\n",
      "      Performance - PLCC: 0.6076, SRCC: 0.5948, KRCC: 0.4226, RMSE: 0.5410\n",
      "\n",
      "    Model: catboost_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_SR.pkl\n",
      "      Performance - PLCC: 0.6149, SRCC: 0.6050, KRCC: 0.4349, RMSE: 0.5365\n",
      "\n",
      "  Target label: S\n",
      "    Training samples: 800, Test samples: 200\n",
      "\n",
      "    Model: mlp_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_S.pkl\n",
      "      Performance - PLCC: 0.7562, SRCC: 0.7684, KRCC: 0.5682, RMSE: 0.5396\n",
      "\n",
      "    Model: ridge_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_S.pkl\n",
      "      Performance - PLCC: 0.7649, SRCC: 0.7848, KRCC: 0.5865, RMSE: 0.5082\n",
      "\n",
      "    Model: decision_tree_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_S.pkl\n",
      "      Performance - PLCC: 0.5317, SRCC: 0.5187, KRCC: 0.3638, RMSE: 0.6955\n",
      "\n",
      "    Model: random_forest_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_S.pkl\n",
      "      Performance - PLCC: 0.7613, SRCC: 0.7549, KRCC: 0.5564, RMSE: 0.5187\n",
      "\n",
      "    Model: extra_trees_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_S.pkl\n",
      "      Performance - PLCC: 0.7545, SRCC: 0.7479, KRCC: 0.5497, RMSE: 0.5260\n",
      "\n",
      "    Model: gradient_boosting_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_S.pkl\n",
      "      Performance - PLCC: 0.7964, SRCC: 0.8027, KRCC: 0.6006, RMSE: 0.4745\n",
      "\n",
      "    Model: adaboost_regressor\n",
      "      Model already exists, loading...\n",
      "      Loaded existing model: level1_U1_S.pkl\n",
      "      Performance - PLCC: 0.7991, SRCC: 0.7958, KRCC: 0.5894, RMSE: 0.4817\n",
      "\n",
      "    Model: svr_regressor\n",
      "      Training svr_regressor...\n",
      "      Performing hyperparameter search...\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Update these paths to match your setup\n",
    "    features_csv_path = \"../preprocessing/scaling/scaled-features/power/all_features_power.csv\"\n",
    "    labels_csv_path = \"../../dataset/cleaned/cleaned-mos.csv\"  # Path to your labels CSV file\n",
    "    models_path = \"./trained_models\"\n",
    "    \n",
    "    # Run training\n",
    "    trainer = run_training(\n",
    "        csv_file_path=features_csv_path,\n",
    "        labels_csv_path=labels_csv_path,\n",
    "        models_save_path=models_path,\n",
    "        test_size=0.2,\n",
    "        force_retrain=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912391a-0e9f-4af8-ac1b-f229e0437928",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
